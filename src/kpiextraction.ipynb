{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KPI Extractor   \n",
        "### A comprehensive workflow to extract important KPIs from financial documents."
      ],
      "metadata": {
        "id": "fIvQCr8oR2sU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Required Libraries"
      ],
      "metadata": {
        "id": "h8UFRA2wn35p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet -U langchain_openai langchain_core langchain_community langgraph pytesseract pdf2image pillow pandas==2.2.2 openpyxl tesseract pypdf"
      ],
      "metadata": {
        "id": "UjhMCY19R8a4"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get install -y tesseract-ocr"
      ],
      "metadata": {
        "id": "JyG5xW_4nlLR"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Required Libraries"
      ],
      "metadata": {
        "id": "zpTJ1lJfojIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "from pprint import pprint\n",
        "from google.colab import userdata\n",
        "from IPython.display import display, Image\n",
        "from typing import Dict, List, Any\n",
        "from typing_extensions import TypedDict, Annotated, Union\n",
        "\n",
        "# PDF processing libraries\n",
        "import io\n",
        "import pytesseract\n",
        "import pandas as pd\n",
        "from pdf2image import convert_from_path\n",
        "\n",
        "# other libraries\n",
        "from pydantic import BaseModel, field_validator, ValidationError, Field\n",
        "\n",
        "# LangChain and LangGraph\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.schema import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_core.messages import AIMessage, HumanMessage, AnyMessage, SystemMessage, ToolMessage, RemoveMessage\n",
        "from langchain.schema.runnable import RunnableConfig\n",
        "\n",
        "import langgraph\n",
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.checkpoint.memory import MemorySaver"
      ],
      "metadata": {
        "id": "3zs1Xq0LZxqI"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Required Variables"
      ],
      "metadata": {
        "id": "tlLXbekNom7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GIRU_OPENAI_API_KEY = userdata.get('GIRU_OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = GIRU_OPENAI_API_KEY\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r'/usr/local/bin/pytesseract'"
      ],
      "metadata": {
        "id": "fcjiU_E5TD-R"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Checks & Auths"
      ],
      "metadata": {
        "id": "5NnT5RuvouXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking openai llm\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "result = llm.invoke(\"Hi, are you deepseek?\")\n",
        "\n",
        "# printing results\n",
        "print(\"===\"*25)\n",
        "print(f\"RESPONSE\\n{result.content}\")\n",
        "print(\"===\"*25)\n",
        "print(\"METADATA\")\n",
        "print(f\"model: {result.response_metadata['model_name']}\")\n",
        "for k, v in result.response_metadata['token_usage'].items():\n",
        "  print(f\"{k}: {v}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNUO-rd3S9p8",
        "outputId": "84c09e50-62a3-42ca-9f1a-f0533fe43d05"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================\n",
            "RESPONSE\n",
            "No, I'm not DeepSeek. I'm an AI language model created by OpenAI. How can I assist you today?\n",
            "===========================================================================\n",
            "METADATA\n",
            "model: gpt-4o-mini-2024-07-18\n",
            "completion_tokens: 25\n",
            "prompt_tokens: 14\n",
            "total_tokens: 39\n",
            "completion_tokens_details: {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}\n",
            "prompt_tokens_details: {'audio_tokens': 0, 'cached_tokens': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class KPIData(TypedDict):\n",
        "    lp_contribution: str\n",
        "    gp_contribution: str\n",
        "    lp_distribution: str\n",
        "    gp_distribution: str\n",
        "    net_irr: str\n",
        "    net_interest: str\n",
        "    lp_nav: str\n",
        "    gp_nav: str\n",
        "    net_debt: str"
      ],
      "metadata": {
        "id": "QgnHx8obZxnM"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WorkflowState(TypedDict):\n",
        "    file_path: str\n",
        "    extracted_text: str\n",
        "    kpi_data: KPIData\n",
        "    human_feedback: str\n",
        "    status: str"
      ],
      "metadata": {
        "id": "SocjaauCY1yE"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(state: WorkflowState) -> WorkflowState:\n",
        "    try:\n",
        "        file_path = state.get(\"file_path\")\n",
        "        print(\"inside load_pdf\\nprinting file path: \", file_path)\n",
        "\n",
        "        loader = PyPDFLoader(file_path)\n",
        "        docs = loader.load()\n",
        "        basic_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        print(f\"loaded pdf successfully!\\nbasic text content: {basic_text[:500]}\")\n",
        "\n",
        "        return {**state, \"extracted_text\": basic_text, \"status\": \"pdf_loaded\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {**state, \"error\": f\"Error loading PDF: {str(e)}\", \"status\": \"error\"}"
      ],
      "metadata": {
        "id": "FVHCwf6KanGr"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(path):\n",
        "    file_path = path\n",
        "    print(\"inside load_pdf\\nprinting file path: \", file_path)\n",
        "\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    docs = loader.load()\n",
        "    basic_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    print(f\"loaded pdf successfully!\\nbasic text content: {basic_text[:500]}\")\n",
        "\n",
        "load_pdf(\"/content/sample1.pdf\")"
      ],
      "metadata": {
        "id": "8jg_m6x6x1P9",
        "outputId": "c70bd9e6-a4b6-4aee-c7d1-deec0a16fb0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside load_pdf\n",
            "printing file path:  /content/sample1.pdf\n",
            "loaded pdf successfully!\n",
            "basic text content: JMI EQUITY FUND IX-A, L.P.     \n",
            "JMI EQUITY FUND IX-B, L.P. \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "First Quarter Report \n",
            "March 31, 2023\n",
            "JMI EQUITY FUND IX-A, L.P.  \n",
            "JMI EQUITY FUND IX-B, L.P. \n",
            "First Quarter Report \n",
            " \n",
            " \n",
            "Table of Contents \n",
            " \n",
            " \n",
            " \n",
            "I. Summary of Fund Activity \n",
            " \n",
            "II. Portfolio Company Summaries \n",
            " \n",
            "III. Financial Statements \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "   \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "THIS INFORMATION IS CONFIDENTIAL AND PROPRIETARY AND MUST BE MAINTAINED IN \n",
            "CONFIDENCE AND IN ACCORDANCE WITH THE CONFIDENTIALITY OBLIGATIONS \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pdf_text(state: WorkflowState) -> WorkflowState:\n",
        "    try:\n",
        "        file_path = state.get(\"file_path\")\n",
        "        basic_text = state.get(\"extracted_text\", \"\")\n",
        "\n",
        "        # Use OCR for images in PDF\n",
        "        images = convert_from_path(file_path)\n",
        "        ocr_text = \"\"\n",
        "\n",
        "        for i, image in enumerate(images):\n",
        "            text = pytesseract.image_to_string(image)\n",
        "            ocr_text += f\"\\n--- Page {i+1} OCR ---\\n{text}\"\n",
        "\n",
        "        print(f\"completed ocr\\nocr text content: {ocr_text[:300]}\")\n",
        "\n",
        "        # Combine both extraction methods\n",
        "        combined_text = f\"{basic_text}\\n\\n{ocr_text}\"\n",
        "\n",
        "        print(f\"text extracted successfully!\\n{combined_text[:500]}\")\n",
        "\n",
        "        return {**state, \"extracted_text\": combined_text, \"status\": \"text_extracted\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {**state, \"error\": f\"Error extracting text: {str(e)}\", \"status\": \"error\"}"
      ],
      "metadata": {
        "id": "M9MMZam-bLD_"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_kpis(state: WorkflowState) -> WorkflowState:\n",
        "    try:\n",
        "        text_content = state.get(\"extracted_text\", \"\")\n",
        "        human_feedback = state.get(\"human_feedback\", \"\")\n",
        "\n",
        "        # prompt with human feedback\n",
        "        prompt = f\"\"\"\n",
        "        You are a financial document analysis expert. Extract the following KPIs from the provided text content.\n",
        "        If a value is not found, mark it as 'Not Found'.\n",
        "\n",
        "        Text content from PDF:\n",
        "        ```\n",
        "        {text_content}\n",
        "        ```\n",
        "\n",
        "        Extract the following KPIs:\n",
        "        1. LP Contribution\n",
        "        2. GP Contribution\n",
        "        3. LP Distribution\n",
        "        4. GP Distribution\n",
        "        5. Net IRR\n",
        "        6. Net Interest\n",
        "        7. LP NAV\n",
        "        8. GP NAV\n",
        "        9. Net Debt\n",
        "\n",
        "        IMPORTANT HUMAN FEEDBACK TO CONSIDER:\n",
        "        {human_feedback}\n",
        "\n",
        "        Return your findings which must be inclosed inside ```json\\n(your_findings as json dictionary)\\n``` as a JSON object with these keys (keys must be inside inverted commas and with the same name as below):\n",
        "        lp_contribution, gp_contribution, lp_distribution, gp_distribution, net_irr, net_interest, lp_nav, gp_nav, net_debt\n",
        "        \"\"\"\n",
        "\n",
        "        # Extract KPIs using LLM\n",
        "        response = llm.invoke([HumanMessage(content=prompt)])\n",
        "\n",
        "        # Try to parse the JSON from the response\n",
        "        try:\n",
        "            # Look for JSON in the response\n",
        "            response_text = response.content\n",
        "\n",
        "            # Extract JSON part if embedded in text explanation\n",
        "            json_match = re.search(r'```json\\n(.*?)\\n```', response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1)\n",
        "            else:\n",
        "                # Try to find JSON-like structure\n",
        "                json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "                if json_match:\n",
        "                    json_str = json_match.group(0)\n",
        "                else:\n",
        "                    json_str = response_text\n",
        "\n",
        "            kpi_data = json.loads(json_str)\n",
        "\n",
        "            # Ensure all required keys are present\n",
        "            required_keys = [\"lp_contribution\", \"gp_contribution\", \"lp_distribution\", \"gp_distribution\",\n",
        "                            \"net_irr\", \"net_interest\", \"lp_nav\", \"gp_nav\", \"net_debt\"]\n",
        "\n",
        "            for key in required_keys:\n",
        "                if key not in kpi_data:\n",
        "                    kpi_data[key] = \"Not Found\"\n",
        "\n",
        "        except Exception as parse_error:\n",
        "            print(f\"Error parsing LLM response: {str(parse_error)}\")\n",
        "            print(f\"Raw response: {response.content}\")\n",
        "\n",
        "            # Use previous KPI data if parsing fails\n",
        "            kpi_data = state.get(\"kpi_data\", {})\n",
        "\n",
        "        print(f\"Extracted KPIs\")\n",
        "\n",
        "        return {**state, \"kpi_data\": kpi_data, \"status\": \"kpis_reextracted\"}\n",
        "    except Exception as e:\n",
        "        return {**state, \"error\": f\"Error re-extracting KPIs: {str(e)}\", \"status\": \"error\"}"
      ],
      "metadata": {
        "id": "Z5JsYhH0dAF0"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def human_validation(state: WorkflowState) -> Union[str, Dict]:\n",
        "    try:\n",
        "        kpi_data = state.get(\"kpi_data\", {})\n",
        "\n",
        "        # Format KPI data for display\n",
        "        kpi_display = \"\\n\".join([f\"{k.upper()}: {v}\" for k, v in kpi_data.items()])\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"EXTRACTED KPIs FOR VALIDATION:\")\n",
        "        print(kpi_display)\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Get human feedback\n",
        "        choice = input(\"\\nAccept these KPIs? (yes/no): \").strip().lower()\n",
        "\n",
        "        if choice == \"yes\":\n",
        "            print(\"✅ KPIs accepted!\")\n",
        "            return \"save_to_excel\"\n",
        "        else:\n",
        "            feedback = input(\"Please provide feedback for improving extraction: \")\n",
        "            return {\"feedback\": feedback}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {**state, \"error\": f\"Error during human validation: {str(e)}\", \"status\": \"error\"}"
      ],
      "metadata": {
        "id": "LyLOjbGscbD6"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_human_feedback(state: WorkflowState, feedback_data: Dict) -> WorkflowState:\n",
        "    feedback = feedback_data.get(\"feedback\", \"\")\n",
        "\n",
        "    # Update state with feedback\n",
        "    updated_state = {\n",
        "        **state,\n",
        "        \"human_feedback\": feedback,\n",
        "        \"status\": \"feedback_received\"\n",
        "    }\n",
        "\n",
        "    print(f\"✅ Human feedback recorded: {feedback}\")\n",
        "\n",
        "    return updated_state"
      ],
      "metadata": {
        "id": "EDYQkWxockYP"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_excel(state: WorkflowState) -> WorkflowState:\n",
        "    try:\n",
        "        kpi_data = state.get(\"kpi_data\", {})\n",
        "        file_path = state.get(\"file_path\", \"defaultname\")\n",
        "\n",
        "        # Create filename based on input PDF name\n",
        "        base_name = os.path.basename(file_path)\n",
        "        pdf_name = os.path.splitext(base_name)[0]\n",
        "        excel_path = f\"{pdf_name}_KPIs.xlsx\"\n",
        "\n",
        "        # Convert KPI data to DataFrame\n",
        "        df = pd.DataFrame([kpi_data])\n",
        "\n",
        "        # Save to Excel\n",
        "        df.to_excel(excel_path, index=False)\n",
        "\n",
        "        print(f\"✅ Saved KPIs to Excel: {excel_path}\")\n",
        "\n",
        "        return {**state, \"status\": \"completed\", \"excel_path\": excel_path}\n",
        "\n",
        "    except Exception as e:\n",
        "        return {**state, \"error\": f\"Error saving to Excel: {str(e)}\", \"status\": \"error\"}"
      ],
      "metadata": {
        "id": "8ap8Rly2czw8"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_workflow() -> StateGraph:\n",
        "    # Initialize the graph\n",
        "    workflow = StateGraph(WorkflowState)\n",
        "\n",
        "    # Add nodes\n",
        "    workflow.add_node(\"load_pdf\", load_pdf)\n",
        "    workflow.add_node(\"extract_pdf_text\", extract_pdf_text)\n",
        "    workflow.add_node(\"extract_kpis\", extract_kpis)\n",
        "    workflow.add_node(\"human_validation\", human_validation)\n",
        "    workflow.add_node(\"process_human_feedback\", process_human_feedback)\n",
        "    workflow.add_node(\"save_to_excel\", save_to_excel)\n",
        "\n",
        "    # Define edges for sequential flow\n",
        "    workflow.add_edge(\"load_pdf\", \"extract_pdf_text\")\n",
        "    workflow.add_edge(\"extract_pdf_text\", \"extract_kpis\")\n",
        "    workflow.add_edge(\"extract_kpis\", \"human_validation\")\n",
        "    workflow.add_conditional_edges(\n",
        "        \"human_validation\",\n",
        "        {\n",
        "            \"save_to_excel\": lambda x: x == \"save_to_excel\",\n",
        "            \"process_human_feedback\": lambda x: isinstance(x, dict) and \"feedback\" in x\n",
        "        }\n",
        "    )\n",
        "    workflow.add_edge(\"process_human_feedback\", \"extract_kpis\")\n",
        "    workflow.add_edge(\"extract_kpis\", \"human_validation\")\n",
        "    workflow.add_edge(\"save_to_excel\", END)\n",
        "\n",
        "    # Set the entry point\n",
        "    workflow.set_entry_point(\"load_pdf\")\n",
        "\n",
        "    return workflow"
      ],
      "metadata": {
        "id": "3CUnF84pdv42"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_workflow(pdf_path: str):\n",
        "    workflow = build_workflow()\n",
        "    app = workflow.compile()\n",
        "    # display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
        "    return app"
      ],
      "metadata": {
        "id": "rkJrvxCdfD3A"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_workflow(pdf_path: str):\n",
        "    app = compile_workflow(pdf_path)\n",
        "\n",
        "    # Initial state\n",
        "    initial_state = WorkflowState(\n",
        "        file_path=pdf_path,\n",
        "        extracted_text=\"\",\n",
        "        kpi_data={},\n",
        "        human_feedback=\"\",\n",
        "        status=\"started\",\n",
        "        error=\"\"\n",
        "    )\n",
        "\n",
        "    # Execute the workflow\n",
        "    for output in app.stream(initial_state):\n",
        "        node = output.get(\"node\")\n",
        "        if node == \"error\":\n",
        "            print(f\"❌ Error: {output['state'].get('error')}\")\n",
        "            return output['state']\n",
        "\n",
        "    # Return the final state\n",
        "    final_state = output.get(\"state\", {})\n",
        "    return final_state"
      ],
      "metadata": {
        "id": "etmbV_upfNcR"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the workflow on the uploaded PDF\n",
        "result = run_workflow(\"/content/sample1.pdf\")\n",
        "\n",
        "# Check the result\n",
        "if result.get(\"status\") == \"completed\":\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"✅ WORKFLOW COMPLETED SUCCESSFULLY\")\n",
        "    print(f\"📊 KPIs saved to: {result.get('excel_path')}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Print extracted KPIs\n",
        "    print(\"\\nExtracted KPIs:\")\n",
        "    kpi_data = result.get(\"kpi_data\", {})\n",
        "    for key, value in kpi_data.items():\n",
        "        print(f\"  - {key.upper()}: {value}\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"❌ WORKFLOW FAILED: {result.get('error')}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XLt0wLQeiSTg",
        "outputId": "1bb14b8c-4eb9-4fb6-dd7c-ffc9e8f63da1"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside load_pdf\n",
            "printing file path:  /content/sample1.pdf\n",
            "loaded pdf successfully!\n",
            "basic text content: JMI EQUITY FUND IX-A, L.P.     \n",
            "JMI EQUITY FUND IX-B, L.P. \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "First Quarter Report \n",
            "March 31, 2023\n",
            "JMI EQUITY FUND IX-A, L.P.  \n",
            "JMI EQUITY FUND IX-B, L.P. \n",
            "First Quarter Report \n",
            " \n",
            " \n",
            "Table of Contents \n",
            " \n",
            " \n",
            " \n",
            "I. Summary of Fund Activity \n",
            " \n",
            "II. Portfolio Company Summaries \n",
            " \n",
            "III. Financial Statements \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "   \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "THIS INFORMATION IS CONFIDENTIAL AND PROPRIETARY AND MUST BE MAINTAINED IN \n",
            "CONFIDENCE AND IN ACCORDANCE WITH THE CONFIDENTIALITY OBLIGATIONS \n",
            "Extracted KPIs\n",
            "\n",
            "==================================================\n",
            "EXTRACTED KPIs FOR VALIDATION:\n",
            "LP_CONTRIBUTION: 1273050007\n",
            "GP_CONTRIBUTION: 11658721\n",
            "LP_DISTRIBUTION: 484721771\n",
            "GP_DISTRIBUTION: Not Found\n",
            "NET_IRR: 26\n",
            "NET_INTEREST: 309555\n",
            "LP_NAV: 1724498074\n",
            "GP_NAV: 312035219\n",
            "NET_DEBT: 31556080\n",
            "==================================================\n",
            "\n",
            "Accept these KPIs? (yes/no): yes\n",
            "✅ KPIs accepted!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidUpdateError",
          "evalue": "Expected dict, got save_to_excel\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-f81100bbb1a0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the workflow on the uploaded PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_workflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/sample1.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Check the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"completed\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-150-fd90bc6a8f7e>\u001b[0m in \u001b[0;36mrun_workflow\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Execute the workflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"node\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/__init__.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   2029\u001b[0m                 \u001b[0;31m# with channel updates applied only at the transition between steps.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2030\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2031\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2032\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2033\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    231\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/utils/runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_set_config_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/write.py\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         ]\n\u001b[0;32m---> 93\u001b[0;31m         self.do_write(\n\u001b[0m\u001b[1;32m     94\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/pregel/write.py\u001b[0m in \u001b[0;36mdo_write\u001b[0;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTASKS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteTupleEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mww\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                     \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChannelWriteEntry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langgraph/graph/state.py\u001b[0m in \u001b[0;36m_get_updates\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0merror_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_GRAPH_NODE_RETURN_VALUE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                 )\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidUpdateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;31m# state updaters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got save_to_excel\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_GRAPH_NODE_RETURN_VALUE"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "34EglI-_kv5U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}